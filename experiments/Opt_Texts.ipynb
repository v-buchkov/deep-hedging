{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41VE08_pyFuR"
   },
   "source": [
    "# **Deep Hedging**\n",
    "# Buchkov Viacheslav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LKcSNj4tlRVK"
   },
   "source": [
    "import abc\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# You may add any imports you need\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "RANDOM_SEED = 12"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "47YPLjDL-Mtv"
   },
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(RANDOM_SEED)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZrCB4LgfO8y6"
   },
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmkSjI1Regtf",
    "outputId": "c57a6b5f-8909-4f79-93d0-8f4b9eee7d30"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "\n",
    "ROOT_PATH = Path(\"dataset\")\n",
    "PATH = Path(\"/content/gdrive/MyDrive/\")\n",
    "N_DAYS = 5"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kmiS11dsOnaq"
   },
   "source": [
    "# data = pd.read_pickle(PATH / \"final_df.pkl\")\n",
    "# # data[\"rub_rate\"] = data[\"rub_rate\"] / 100\n",
    "# data.dropna(inplace=True)\n",
    "# data = data.resample(\"1 min\").ffill()\n",
    "# data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8UKB-Db2PqIK"
   },
   "source": [
    "import abc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class PositionSide(Enum):\n",
    "    LONG = 1\n",
    "    SHORT = -1\n",
    "\n",
    "\n",
    "\n",
    "class Instrument:\n",
    "    CALENDAR_DAYS: int = 365\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def bid(self, margin: float) -> float:\n",
    "        return self.price() - margin\n",
    "\n",
    "    def offer(self, margin: float) -> float:\n",
    "        return self.price() + margin\n",
    "\n",
    "    @staticmethod\n",
    "    def discount_factor(rate: float, term: float) -> float:\n",
    "        return np.exp(-rate * term)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def coupon(self, frequency: float = 0., *args, **kwargs) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def pv_coupons(self) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def price(self, spot_start: [float, list[float], None] = None) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def payoff(self, spot: [np.array, float]) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __repr__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return StructuredNote([(PositionSide.LONG, self), (PositionSide.LONG, other)])\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return StructuredNote([(PositionSide.LONG, self), (PositionSide.SHORT, other)])\n",
    "\n",
    "\n",
    "class StructuredNote:\n",
    "    def __init__(self, instruments: [list[tuple[PositionSide, Instrument]], None] = None):\n",
    "        if instruments is not None:\n",
    "            self.instruments = instruments\n",
    "        else:\n",
    "            self.instruments = []\n",
    "\n",
    "    def bid(self, margin: float) -> float:\n",
    "        return self.price() - margin\n",
    "\n",
    "    def offer(self, margin: float) -> float:\n",
    "        return self.price() + margin\n",
    "\n",
    "    def coupon(self, frequency: float = 0., commission: float = 0., *args, **kwargs) -> float:\n",
    "        return sum([instrument.coupon(frequency, commission) for _, instrument in self.instruments])\n",
    "\n",
    "    def __add__(self, other: Instrument):\n",
    "        return self.instruments.append((PositionSide.LONG, other))\n",
    "\n",
    "    def __sub__(self, other: Instrument):\n",
    "        return self.instruments.append((PositionSide.SHORT, other))\n",
    "\n",
    "    def price(self) -> float:\n",
    "        return sum([side.value * instrument.price() + instrument.pv_coupons() for side, instrument in self.instruments])\n",
    "\n",
    "    def payoff(self, spot_paths: np.array) -> float:\n",
    "        return sum([side.value * instrument.payoff(spot_paths) for side, instrument in self.instruments])\n",
    "\n",
    "    def __repr__(self):\n",
    "        sp_str = f\"StructuredNote of:\\n\"\n",
    "        for side, instrument in self.instruments:\n",
    "            sp_str += f\"* {side} -> {instrument}\\n\"\n",
    "        return sp_str\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WSq4YrPxPsBP"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Forward(Instrument):\n",
    "    def __init__(\n",
    "            self,\n",
    "            rates_difference: float,\n",
    "            spot_price: float,\n",
    "            term: float,\n",
    "            *args,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rates_difference = rates_difference\n",
    "        self.spot_price = spot_price\n",
    "        self.term = term\n",
    "\n",
    "    def coupon(self, frequency: float = 0., *args, **kwargs) -> float:\n",
    "        return 0\n",
    "\n",
    "    def pv_coupons(self) -> float:\n",
    "        return 0\n",
    "\n",
    "    def get_strike(self, spot_price: [float, None] = None) -> float:\n",
    "        if spot_price is None:\n",
    "            spot_price = self.spot_price\n",
    "        return spot_price * self.discount_factor(rate=-self.rates_difference, term=self.term)\n",
    "\n",
    "    @property\n",
    "    def strike(self) -> float:\n",
    "        return self.get_strike()\n",
    "\n",
    "    def price(self, spot_start: [float, list[float], None] = None) -> float:\n",
    "        return 0\n",
    "\n",
    "    def payoff(self, spot: [float, np.array]) -> float:\n",
    "        return spot - self.strike\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Forward(strike={self.strike}, term={self.term}, spot_ref={self.spot_price})\"\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Na70Ea_A7hoi"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class EuropeanCall(Instrument):\n",
    "    def __init__(\n",
    "            self,\n",
    "            rates_difference: float,\n",
    "            spot_price: float,\n",
    "            term: float,\n",
    "            *args,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rates_difference = rates_difference\n",
    "        self.spot_price = spot_price\n",
    "        self.term = term\n",
    "\n",
    "    def coupon(self, frequency: float = 0., *args, **kwargs) -> float:\n",
    "        return 0\n",
    "\n",
    "    def pv_coupons(self) -> float:\n",
    "        return 0\n",
    "\n",
    "    def get_strike(self, spot_price: [float, None] = None) -> float:\n",
    "        return self.spot_price\n",
    "\n",
    "    @property\n",
    "    def strike(self) -> float:\n",
    "        return self.get_strike()\n",
    "\n",
    "    def price(self, spot_start: [float, list[float], None] = None) -> float:\n",
    "        return 0\n",
    "\n",
    "    def payoff(self, spot: [float, np.array]) -> float:\n",
    "        return max(spot - self.strike, 0)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"EuropeanCall(strike={self.strike}, term={self.term}, spot_ref={self.spot_price})\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Mou9GCV8Onar"
   },
   "source": [
    "# from src.base.instrument import Instrument\n",
    "# from src.forward.forward import Forward\n",
    "\n",
    "\n",
    "def create_instrument(period_df: pd.DataFrame) -> Instrument:\n",
    "    start = period_df.loc[period_df.index.min()]\n",
    "    return EuropeanCall(\n",
    "        rates_difference=start[\"rub_rate\"] - start[\"usd_rate\"],\n",
    "        spot_price=start[\"ask\"],\n",
    "        term=N_DAYS / 365\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# data = pd.read_pickle(PATH / \"final_df.pkl\")"
   ],
   "metadata": {
    "id": "aOGFbzI-VL6o"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# tqdm.pandas()\n",
    "# data.loc[data[\"embed\"].map(len) < 128, \"embed\"] = data[data[\"embed\"].map(len) < 128][\"embed\"].progress_apply(lambda embed: np.pad(embed, (0, 128 - len(embed)), 'constant', constant_values=0))"
   ],
   "metadata": {
    "id": "acD4pTDeVOq7"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# data[data[\"embed\"].map(len) < 128]"
   ],
   "metadata": {
    "id": "cbD32_jvV5_Z"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# tqdm.pandas()\n",
    "# data = pd.read_pickle(PATH / \"final_df.pkl\")\n",
    "# data[\"padded\"] = data[\"embed\"].progress_apply(lambda embed: np.pad(embed, (0, 128 - len(embed)), 'constant', constant_values=0))\n",
    "# data"
   ],
   "metadata": {
    "id": "Wi_SJt2OS5ld"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# data.to_pickle(PATH / \"padded.pkl\")"
   ],
   "metadata": {
    "id": "v9S-6bpwTdpp"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cty5h8mOOnas"
   },
   "source": [
    "## Dataset."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "MAX_LENGTH = 128"
   ],
   "metadata": {
    "id": "7FtyavwdMpPX"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HVpgB39EzM9H"
   },
   "source": [
    "import datetime as dt\n",
    "\n",
    "from typing import Union, Type\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from src.base.instrument import Instrument\n",
    "\n",
    "\n",
    "class SpotDataset(Dataset):\n",
    "    BID_COLUMN: str = \"bid\"\n",
    "    ASK_COLUMN: str = \"ask\"\n",
    "    RATE_DOMESTIC_COLUMN: str = \"rub_rate\"\n",
    "    RATE_FOREIGN_COLUMN: str = \"usd_rate\"\n",
    "\n",
    "    TRADING_DAYS: int = 252\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            instrument_cls: Type[Instrument],\n",
    "            filename: str = \"final_df\",\n",
    "            n_days: int = N_DAYS,\n",
    "            path: Path = PATH,\n",
    "            data: Union[pd.DataFrame, None] = None,\n",
    "            max_length: int = MAX_LENGTH\n",
    "    ):\n",
    "        self.filename = filename\n",
    "\n",
    "        self.instrument_cls = instrument_cls\n",
    "        self.n_days = n_days\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.data = self._create_df(path) if data is None else data.copy()\n",
    "\n",
    "        self.data[\"time_diff\"] = self.data.index.to_series().diff()\n",
    "        self.data.loc[self.data.index[0], \"time_diff\"] = pd.to_timedelta(\"0 days 00:00:00\")\n",
    "        self.data[\"time_diff\"] = self.data[\"time_diff\"].cumsum() / np.timedelta64(1, 'D') / 365\n",
    "        # self.data[\"rub_rate\"] = self.data[\"rub_rate\"] / 100\n",
    "        self.data.drop([\"text\", \"lemmas\"], axis=1, inplace=True)\n",
    "\n",
    "        # self.data = self.data.dropna()\n",
    "        self.data = self.data.resample(\"30 min\").ffill()\n",
    "        self.data = self.data.dropna()\n",
    "        # self.X, self.y = self._create_dataset()\n",
    "\n",
    "    def _create_df(self, path: Path) -> pd.DataFrame:\n",
    "        if f\"{self.filename}.pkl\" in os.listdir(path):\n",
    "            return pd.read_pickle(PATH / f\"{self.filename}.pkl\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def _create_instrument(self, period_df: pd.DataFrame) -> Instrument:\n",
    "        start = period_df.loc[period_df.index.min()]\n",
    "        spot_start = (start[\"bid\"] + start[\"ask\"]) / 2\n",
    "        return self.instrument_cls(\n",
    "            rates_difference=start[\"rub_rate\"] - start[\"usd_rate\"],\n",
    "            spot_price=spot_start,\n",
    "            term=N_DAYS / 365\n",
    "        ), spot_start\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[self.data.index < self.data.index.max() - dt.timedelta(days=self.n_days)])\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        start_date = self.data.index[idx]\n",
    "        end_date = start_date + dt.timedelta(days=self.n_days)\n",
    "\n",
    "        features = self.data[(self.data.index >= start_date) & (self.data.index <= end_date)].copy()\n",
    "        features[\"time_diff\"] = features.iloc[-1, -1] - features[\"time_diff\"]\n",
    "\n",
    "        try:\n",
    "            embeds = torch.stack(features[\"embed\"].tolist())[:, :128, :]\n",
    "        except:\n",
    "            print(features[\"embed\"].tolist())\n",
    "        features.drop([\"embed\"], axis=1, inplace=True)\n",
    "\n",
    "        instrument, spot_start = self._create_instrument(features)\n",
    "        # features[\"spot_start\"] = spot_start\n",
    "        target = instrument.payoff(spot=features.ask.iloc[-1])\n",
    "\n",
    "        return torch.Tensor(features.to_numpy()).to(torch.float32), torch.Tensor(embeds).to(torch.float32), torch.Tensor([target]).to(torch.float32)\n",
    "\n",
    "    @property\n",
    "    def average_dt(self):\n",
    "        return self.data.index.to_series().diff(1).mean() / (np.timedelta64(1, 'D') * self.TRADING_DAYS)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uARQpAIhOnat",
    "outputId": "c1645cd6-2fb9-4640-b32e-0bd398ce9157"
   },
   "source": [
    "spot_dataset = SpotDataset(filename=\"final_df\", instrument_cls=EuropeanCall)\n",
    "spot_dataset[-2][1].shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSRMeUecOnau",
    "outputId": "8f3ee2cd-c566-4bb5-a932-0c2e0b1969e8"
   },
   "source": [
    "AVERAGE_DT = spot_dataset.average_dt\n",
    "AVERAGE_DT"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# spot_dataset[101][0].shape"
   ],
   "metadata": {
    "id": "AUMv6VXTIVF7"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oNFI-9syOnav"
   },
   "source": [
    "def get_pnl(spot: torch.Tensor, weights: torch.Tensor, dt: float = AVERAGE_DT) -> torch.float32:\n",
    "    model_device = spot.device\n",
    "    weights_all = torch.concat([torch.zeros(spot.shape[0], 1, requires_grad=False).to(model_device), weights,\n",
    "                                torch.zeros(spot.shape[0], 1, requires_grad=False).to(model_device)], dim=1)\n",
    "    weights_diff = weights_all.diff(n=1, dim=1)\n",
    "\n",
    "    rates_diff = spot[:, :, 2] - spot[:, :, 3]\n",
    "\n",
    "    bought = torch.where(weights_diff > 0, weights_diff, 0)\n",
    "    sold = torch.where(weights_diff < 0, weights_diff, 0)\n",
    "\n",
    "    interest = (rates_diff * -weights_all).sum(dim=1) * dt\n",
    "\n",
    "    cash_outflow = (-spot[:, 1:, 1] * bought).sum(dim=1)\n",
    "    cash_inflow = (-spot[:, 1:, 0] * sold).sum(dim=1)\n",
    "\n",
    "    return (cash_outflow + cash_inflow + interest).unsqueeze(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RlPV1uOiLR2q"
   },
   "source": [
    "class NeuralHedger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 5,\n",
    "        num_layers: int = 5,\n",
    "        hidden_size: int = 32,\n",
    "        dt: float = AVERAGE_DT,\n",
    "        embed_size: float = 128,\n",
    "        num_embed: int = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dt = dt\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(241)\n",
    "\n",
    "        self.lstm_text = nn.LSTM(num_embed, 32, num_layers=3, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.sentence = nn.Sequential(\n",
    "            nn.Linear(2 * self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size + embed_size, self.hidden_size, num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.hedging_weights = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, spot: torch.Tensor, text: torch.Tensor, hidden: [(torch.Tensor), None] = None, return_hidden: bool = False) -> [\n",
    "        torch.Tensor,\n",
    "        (torch.Tensor, torch.Tensor, torch.Tensor)]:\n",
    "        model_device = spot.device\n",
    "        if hidden is None:\n",
    "            h_t = torch.zeros(self.num_layers, spot.size(0), self.hidden_size, dtype=torch.float32).to(model_device)\n",
    "            c_t = torch.zeros(self.num_layers, spot.size(0), self.hidden_size, dtype=torch.float32).to(model_device)\n",
    "        elif len(hidden) != 2:\n",
    "            raise ValueError(f\"Expected two hidden state variables, got {len(hidden)}\")\n",
    "        else:\n",
    "            h_t, c_t = hidden\n",
    "\n",
    "        hidden = []\n",
    "        for input_t in text.chunk(text.size(0), dim=0):\n",
    "            h, c = self.lstm_text(input_t.squeeze(0))\n",
    "            x = self.sentence(h)\n",
    "            hidden.append(x.squeeze(1))\n",
    "        hidden = torch.stack(hidden).squeeze(3)\n",
    "\n",
    "        price = self.batch_norm(spot[:, :, :2])\n",
    "        rates = self.batch_norm(spot[:, :, 2:4])\n",
    "        spot = torch.cat([price, rates, spot[:, :, 4:], hidden], dim=2)\n",
    "\n",
    "        h_t, c_t = self.lstm(spot, (h_t, c_t))\n",
    "        outputs = self.hedging_weights(h_t)[:, 1:-1, :].squeeze(2)\n",
    "\n",
    "        if return_hidden:\n",
    "            return outputs, (h_t, c_t)\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "    def get_pnl(self, spot: torch.Tensor, text: torch.Tensor) -> [torch.Tensor, torch.float32]:\n",
    "        # hedging_weights = nn.Softmax()(self.forward(spot, return_hidden=False), dim=XXX)\n",
    "        weights = self.forward(spot, text, return_hidden=False)\n",
    "\n",
    "        model_device = spot.device\n",
    "        weights_all = torch.concat([torch.zeros(spot.shape[0], 1, requires_grad=False).to(model_device), weights,\n",
    "                                    torch.zeros(spot.shape[0], 1, requires_grad=False).to(model_device)], dim=1)\n",
    "        weights_diff = weights_all.diff(n=1, dim=1)\n",
    "\n",
    "        rates_diff = spot[:, :, 2] - spot[:, :, 3]\n",
    "\n",
    "        bought = torch.where(weights_diff > 0, weights_diff, 0)\n",
    "        sold = torch.where(weights_diff < 0, weights_diff, 0)\n",
    "\n",
    "        interest = (rates_diff * -weights_all).sum(dim=1) * self.dt\n",
    "\n",
    "        cash_outflow = (-spot[:, 1:, 1] * bought).sum(dim=1)\n",
    "        cash_inflow = (-spot[:, 1:, 0] * sold).sum(dim=1)\n",
    "\n",
    "        pnl = (cash_outflow + cash_inflow + interest).unsqueeze(1)\n",
    "\n",
    "        return weights, pnl"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# for features, text, target_pnl in loader:\n",
    "#     w = hedger(features.to(DEVICE), text.to(DEVICE))\n",
    "#     break\n",
    "# w"
   ],
   "metadata": {
    "id": "0S6l1pyP7rV0"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# hedger = NeuralHedger().to(DEVICE)\n",
    "# loader = DataLoader(SpotDataset(instrument_cls=EuropeanCall), batch_size=32, shuffle=True, drop_last=False)\n",
    "\n",
    "# for features, text, target_pnl in loader:\n",
    "#     w = hedger(features.to(DEVICE), text.to(DEVICE))\n",
    "#     break\n",
    "# w"
   ],
   "metadata": {
    "id": "iAFnyRsoHECY"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "VfXJSR-XOnaw"
   },
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module, optimizer: torch.optim.Optimizer, criterion: nn.Module,\n",
    "                loader: DataLoader, tqdm_desc: str = \"Model\"):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    if tqdm_desc is None:\n",
    "        iterator = loader\n",
    "    else:\n",
    "        iterator = tqdm(loader, desc=tqdm_desc)\n",
    "\n",
    "    train_loss = 0.0\n",
    "    model_diff = 0.0\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "    pnl_path = []\n",
    "    weight_path = []\n",
    "    diffs_path = []\n",
    "    for features, text, target_pnl in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        features = features.to(device)\n",
    "        text = text.to(device)\n",
    "        target_pnl = target_pnl.to(device)\n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            weights, model_pnl = model.get_pnl(features, text)\n",
    "            loss = criterion(target_pnl, model_pnl)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        scaler.update()\n",
    "\n",
    "        diff = target_pnl - model_pnl\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        model_diff += diff.mean().item()\n",
    "\n",
    "        diffs_path.append(diff.detach().cpu().numpy())\n",
    "        weight_path.append(weights.detach().cpu().numpy())\n",
    "\n",
    "    train_loss /= len(loader.dataset)\n",
    "    model_diff /= len(loader.dataset)\n",
    "\n",
    "    return train_loss, weight_path, model_diff, diffs_path\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_epoch(model: nn.Module, criterion: nn.Module,\n",
    "                     loader: DataLoader, tqdm_desc: [str, None] = None):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    if tqdm_desc is None:\n",
    "        iterator = loader\n",
    "    else:\n",
    "        iterator = tqdm(loader, desc=tqdm_desc)\n",
    "\n",
    "    val_loss = 0.0\n",
    "    model_diff = 0.0\n",
    "    model.eval()\n",
    "    diffs_path = []\n",
    "    weight_path = []\n",
    "    for features, text, target_pnl in iterator:\n",
    "        features = features.to(device)\n",
    "        text = text.to(device)\n",
    "        target_pnl = target_pnl.to(device)\n",
    "\n",
    "        weights, model_pnl = model.get_pnl(features, text)\n",
    "\n",
    "        loss = criterion(target_pnl, model_pnl)\n",
    "        diff = target_pnl - model_pnl\n",
    "\n",
    "        val_loss += loss.item()\n",
    "        model_diff += diff.mean().item()\n",
    "\n",
    "        diffs_path.append(diff.detach().cpu().numpy())\n",
    "        weight_path.append(weights.detach().cpu().numpy())\n",
    "\n",
    "    val_loss /= len(loader.dataset)\n",
    "    model_diff /= len(loader.dataset)\n",
    "\n",
    "    return val_loss, weight_path, model_diff, diffs_path"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Wij0ERxmMoCl"
   },
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def plot_losses(train_losses: list[float], val_losses: list[float], train_pnls: list[float], val_pnls: list[float]):\n",
    "    clear_output()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(13, 4))\n",
    "    axs[0].plot(range(1, len(train_losses) + 1), train_losses, label='train')\n",
    "    axs[0].plot(range(1, len(val_losses) + 1), val_losses, label='val')\n",
    "    axs[0].set_ylabel('loss')\n",
    "\n",
    "    axs[1].plot(range(1, len(train_pnls) + 1), train_pnls, label='train')\n",
    "    axs[1].plot(range(1, len(val_pnls) + 1), val_pnls, label='val')\n",
    "    axs[1].set_ylabel('PnL, RUB')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend()\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7kOgOK7kOnaw"
   },
   "source": [
    "from typing import Tuple, List, Optional, Any\n",
    "\n",
    "\n",
    "def train(model: nn.Module, optimizer: torch.optim.Optimizer, scheduler: Optional[Any],\n",
    "          train_loader: DataLoader, val_loader: DataLoader,\n",
    "          num_epochs: int, print_logs: bool = True) -> Tuple[List[float], List[float]]:\n",
    "    train_losses, val_losses = [], []\n",
    "    train_diffs, val_diffs = [], []\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        if print_logs:\n",
    "            desc_train = f\"Training {epoch}/{num_epochs}\"\n",
    "            desc_val = f\"Validation {epoch}/{num_epochs}\"\n",
    "        else:\n",
    "            desc_train, desc_val = None, None\n",
    "\n",
    "        train_loss, weights, train_diff, train_path = train_epoch(\n",
    "            model, optimizer, criterion, train_loader,\n",
    "            tqdm_desc=desc_train\n",
    "        )\n",
    "        val_loss, weights, val_diff, val_path = validation_epoch(\n",
    "            model, criterion, val_loader,\n",
    "            tqdm_desc=desc_val\n",
    "        )\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_losses += [train_loss]\n",
    "        val_losses += [val_loss]\n",
    "\n",
    "        train_diffs += [train_diff]\n",
    "        val_diffs += [val_diff]\n",
    "\n",
    "        plot_losses(train_losses, val_losses, train_diffs, val_diffs)\n",
    "\n",
    "    return train_losses, val_losses, weights, train_diffs, val_diffs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "S0i34ec91c9v"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def assess_model(model: nn.Module, baseline: nn.Module) -> None:\n",
    "    _, weights, _, model_diff = validation_epoch(model, nn.MSELoss(), val_loader)\n",
    "    _, _, _, baseline_diff = validation_epoch(baseline, nn.MSELoss(), val_loader)\n",
    "\n",
    "    model_diff = np.concatenate(model_diff, axis=0)\n",
    "    baseline_diff = np.concatenate(baseline_diff, axis=0)\n",
    "\n",
    "    # print(weights[-1])\n",
    "    # print(model_diff)\n",
    "\n",
    "    print(f\"Average weight = {weights[-1].mean()}, Weights = [{weights[-1].min()}; {weights[-1].max()}]\")\n",
    "\n",
    "    print(f\"Means: model = {model_diff.mean():.6f}, baseline = {baseline_diff.mean():.6f}\")\n",
    "\n",
    "    print(f\"Stds: model = {model_diff.std():.6f}, baseline = {baseline_diff.std():.6f}\")\n",
    "\n",
    "    print(f\"VaRs 5%: model = {np.abs(np.quantile(model_diff, 0.05)):.6f}, baseline = {np.abs(np.quantile(baseline_diff, 0.05)):.6f}\")\n",
    "\n",
    "    t_value = (model_diff.mean() - baseline_diff.mean()) / np.sqrt(model_diff.std() ** 2 / model_diff.shape[0] + baseline_diff.std() ** 2 / baseline_diff.shape[0])\n",
    "    print(f\"T-stat = {t_value:.6f}\")\n",
    "\n",
    "    bins = np.linspace(-.25, .25, 100)\n",
    "\n",
    "    plt.hist(model_diff, bins, alpha=0.5, label='model')\n",
    "    plt.hist(baseline_diff, bins, alpha=0.5, label='baseline')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h05ZeQ4D7O_2"
   },
   "source": [
    "## Option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "JazkoYw9N04B"
   },
   "source": [
    "class BaselineEuropeanCall(nn.Module):\n",
    "    def __init__(self, dt: float = AVERAGE_DT):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(1, 1, num_layers=1, batch_first=True)\n",
    "        self.dt = dt\n",
    "\n",
    "        self.strike = 1\n",
    "\n",
    "    def _call_delta(self, mid: torch.Tensor, rates: torch.Tensor, terms: torch.Tensor) -> torch.float32:\n",
    "        \"\"\"\n",
    "        Call option delta [dV/dS] via analytical form solution of Black-Scholes-Merton.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        delta : float\n",
    "            Option delta.\n",
    "        \"\"\"\n",
    "        strikes = mid[:, 0] * self.strike\n",
    "        # print(strikes[: -1])\n",
    "        sigma = mid.std(dim=1).unsqueeze(1)\n",
    "        # print(\"***\")\n",
    "        d1 = (torch.log(mid / strikes.unsqueeze(1)) + (rates + sigma ** 2 / 2) * terms) / (sigma * torch.sqrt(terms))\n",
    "        d1 = d1[:, 1:-1]\n",
    "        # print(d1.shape)\n",
    "        # print(\"***\")\n",
    "\n",
    "        cdf_d1 = torch.distributions.normal.Normal(0, 1).cdf(d1)\n",
    "\n",
    "        return cdf_d1\n",
    "\n",
    "    def forward(self, spot: torch.Tensor, text: torch.Tensor = None, return_hidden: bool = False) -> torch.Tensor:\n",
    "        mid = (spot[:, :, 0] + spot[:, :, 1]) / 2\n",
    "        rates = spot[:, :, 2] - spot[:, :, 3]\n",
    "        terms = spot[:, :, 4]\n",
    "        return self._call_delta(mid=mid, rates=rates, terms=terms)\n",
    "\n",
    "    def get_pnl(self, spot: torch.Tensor, text: torch.Tensor) -> torch.float32:\n",
    "        # hedging_weights = nn.Softmax()(self.forward(spot, return_hidden=False), dim=XXX)\n",
    "        hedging_weights = self.forward(spot, text, return_hidden=False)\n",
    "        return hedging_weights, get_pnl(spot=spot, weights=hedging_weights, dt=self.dt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2pAqlUWqJkFl"
   },
   "source": [
    "class BaselineTrivial(nn.Module):\n",
    "    def __init__(self, dt: float = AVERAGE_DT):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(1, 1, num_layers=1, batch_first=True)\n",
    "        self.dt = dt\n",
    "\n",
    "    def forward(self, spot: torch.Tensor, return_hidden: bool = False) -> torch.Tensor:\n",
    "        return torch.Tensor([[0] * (spot.shape[1] - 2)] * spot.shape[0]).to(torch.float32).to(DEVICE)\n",
    "\n",
    "    def get_pnl(self, spot: torch.Tensor) -> torch.float32:\n",
    "        # hedging_weights = nn.Softmax()(self.forward(spot, return_hidden=False), dim=XXX)\n",
    "        hedging_weights = self.forward(spot, return_hidden=False)\n",
    "        return hedging_weights, get_pnl(spot=spot, weights=hedging_weights, dt=self.dt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Npi7sTeaVGKO"
   },
   "source": [
    "# TEST_SIZE = .1\n",
    "# train_data = data.resample(\"30 min\").ffill()\n",
    "# time_split = train_data.index[int(train_data.index.shape[0] * (1 - TEST_SIZE))]\n",
    "# train_df, test_df = train_data[train_data.index <= time_split], train_data[train_data.index > time_split]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "xL-foURJVDKa",
    "outputId": "df45dbe3-ee15-4dbf-f7ae-eee048158195"
   },
   "source": [
    "N_EPOCHS = 30\n",
    "\n",
    "hedger = NeuralHedger().to(DEVICE)\n",
    "\n",
    "train_set = SpotDataset(instrument_cls=EuropeanCall)\n",
    "val_set = SpotDataset(instrument_cls=EuropeanCall)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(hedger.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N_EPOCHS)\n",
    "\n",
    "train_losses, val_losses, weights, train_diffs, val_diffs = train(\n",
    "    model=hedger,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=N_EPOCHS,\n",
    "    print_logs=True\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YjN-rcSVI5R"
   },
   "source": [
    "assess_model(hedger, BaselineEuropeanCall().to(DEVICE))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTA8jSrBVBfl"
   },
   "source": [
    "torch.save(hedger, PATH / \"opt_еучеы.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I22Nnko4U6wi"
   },
   "source": [
    "# 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qi0KX1_Y7K1f"
   },
   "source": [
    "TEST_SIZE = .1\n",
    "train_data = data.resample(\"5 min\").ffill()\n",
    "time_split = train_data.index[int(train_data.index.shape[0] * (1 - TEST_SIZE))]\n",
    "train_df, test_df = train_data[train_data.index <= time_split], train_data[train_data.index > time_split]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "jsPzA3t66iEy",
    "outputId": "a5f74954-b41b-44f4-bccb-c97be51431cd"
   },
   "source": [
    "N_EPOCHS = 30\n",
    "\n",
    "hedger = NeuralHedger().to(DEVICE)\n",
    "\n",
    "train_set = SpotDataset(data=train_df, instrument_cls=EuropeanCall)\n",
    "val_set = SpotDataset(data=test_df, instrument_cls=EuropeanCall)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True, drop_last=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(hedger.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=N_EPOCHS)\n",
    "\n",
    "train_losses, val_losses, weights, train_diffs, val_diffs = train(\n",
    "    model=hedger,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=N_EPOCHS,\n",
    "    print_logs=True\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "2OKann_46ine",
    "outputId": "d050096a-f5be-4246-e2e3-42e125910e44"
   },
   "source": [
    "assess_model(hedger, BaselineEuropeanCall().to(DEVICE))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkGZ3kuULB55"
   },
   "source": [
    "torch.save(hedger, PATH / \"opt_out_new_30_bn_till_sgd.pt\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "max_cell_id": 35
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
